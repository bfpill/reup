---
title: "2024-11-05"
time:
  created: "2024-11-05T19:24:46.167Z"
  updated: "2024-11-05T06:19:19.634Z"
---

# 2024-11-05

<div className="mt-5 mb-5">
  <SummerDaysGraph day={"2024-11-05"}/>
</div>

9:00 - Worked for a little while on the ARENA mech interp chapters while messing with the CA training. I got impatient with the model and paused it to try out
training with differentiable update rules (done using sigmoid), but that didn't really work out super well, low loss but model was clearly not learning so something was wrong. 
Also tried some very poor RL where I would sample from the logits as actions and then try to use that in the loss function, but it didn't work at all (all this to get around non-differentiable update rules). 
I think just input/output pairs is the way to go, maybe a big enough dataset is all you need. 

<div className="flex flex-row w-full h-40 mt-10 mb-32">
  ![good-loss-bad-acc](/images/daily-notes/2024-11-5/good-loss-bad-acc.png)
</div>

1:11 - Just finished up polishing the site so daily posts are all bundled together in one big scroll. All posts are concatenated into a huge MDX and rolled together. 
It makes the posts much more readable as work can flow over from one day to the next. Might change up the landing page to make it more homely. 

1->5:00 - improved the Lorentz attractor on the homepage so that it spins and has a variable resolution. 

<div className="flex flex-row w-full h-40 mt-10 mb-64">
  ![conways grok](/images/daily-notes/2024-11-5/Lorentz.png)
</div>
<div className="flex flex-row w-full h-40 mt-10 mb-16"/>

6->8:00 - went back to the model training and ended up scrapping the differentiable reversing. Seems like gradients just don't flow well even when using a very soft
approximation of the update rules. Now I want to see if models trained to predict the _next_ board can be run with gradient ascent to find an input board that 
leads to a target board. Probably wont work well, but it will be interesting to mess with L1 and other penalties as the problem is roughly like other many->one
problems. 

Anyways, turns out training models to calc forward like this gives quite nice grokking behaviour over different board sizes. Call it Conways Grok. 

<div className="flex flex-row w-full h-40 mt-10 mb-64">
  ![conways grok](/images/daily-notes/2024-11-5/conways-grok.png)
</div>
<div className="flex flex-row w-full h-40 mb-6"/>

These models are accurate enough (>99.99%) to sample autoregressively. So we can run Conways with a model!

First two are on a slightly undertrained 12x12 model - the glider messes up at the end!
Last is on a 18x18 model. 
<div className="flex flex-row mt-16 mb-32 gap-10">

<div className="flex flex-row w-1/3 h-1/3 ">
  ![conways grok](/images/daily-notes/2024-11-5/neural_glider.gif)
</div>
<div className="flex flex-row w-1/3 h-1/3">
  ![conways grok](/images/daily-notes/2024-11-5/neural12x12.gif)
</div>
<div className="flex flex-row w-1/3 h-1/3">
  ![conways grok](/images/daily-notes/2024-11-5/18x18.gif)
</div>
</div>





