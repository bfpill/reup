---
title: "LLC of Tracr Models Report"
time:
  created: "2024-12-11T00:00:00.000Z"
  updated: "2024-12-11T00:00:00.000Z"
---

These are some results from experiments measuring the LLC of Tracr models.

**To summarize:**

- Tracr models have finicky loss landscapes that make calculating the LLC difficult.
  -  Distilling a Tracr model into a model of equal size or smaller significantly smooths its loss landscape.
- Decreasing model size through distillation (training smaller models to match Tracr outputs) reveals significant LLC variation between models of different sizes.
- Increasing model size through direct compilation (compiling the same program with larger maximum sequence lengths) yields more consistent LLC estimates.

# Tracr Programs Used

We run experiments on the Tracr programs listed below, all sourced from the Tracr git repository. We use the `estimate_learning_coeff` function with SGLD from the devinterp library for all of these experiments. *Note we do not use `dyck-2` and `dyck-3`, as estimating their LLC was very difficult (more in 'appendix' + below).*

| Function        | Rasp Time | Theoretical Time | Description                                    |
|-----------------|-----------|-------------------|------------------------------------------------|
| `length`        | O(n²)     | O(1)              | Outputs sequence length at each position       |
| `frac_prevs`    | O(n²)     | O(n)              | Fraction of previous tokens matching condition |
| `sort`          | O(n²)     | O(n log n)        | Sorts with duplicate keys allowed              |
| `sort_unique`   | O(n²)     | O(n log n)        | Sorts with unique keys                         |
| `hist`          | O(n²)     | O(n)              | Counts token frequencies                       |
| `sort_freq`     | O(n²)     | O(n)              | Sorts by token frequency                       |
| `pair_balance`  | O(n²)     | O(n)              | Tracks balance of open/close tokens            |
| **Not used:**   |           |                   |                                                |
| `dyck-2`        | O(n²)     | O(n)              | Checks balanced `()` and `{}`                  |
| `dyck-3`        | O(n²)     | O(n)              | Checks balanced `()`, `{}`, and `[]`           |

*Note:* 'Rasp time' is always O(n²) because Rasp makes extensive use of the `select` primitive, which creates an n×n selector matrix comparing every position with every other position.

First, we plot the LLC over **compiled** Tracr models of increasing size. We do this by increasing the "Maximum Sequence Length" of the model, which scales the residual stream width and number of layers of our model. We calculate the LLC for models with sequence lengths between 4 and 100. We see that for some tasks LLCs stay constant, while for other tasks the LLCs scale with seq len. To keep relative LLCs fair, all LLCs in the below plot were calculated with the same hyperparameters (1e-5 and default nbeta).

We include lines indicating the **Sparsity Ratio** of the model, which is $\text{Nonzero Params} / \text{Total Params}$.

# LLC vs Sequence Length and **Sparsity Ratio**

<img
  src="/images/tracr-llc/llc_sparsity_comparison_from_csv_5.png"
  alt="LLC vs Sparsity Comparison"
  style={{ display: 'block', width: '150%', marginLeft: '-25%', maxWidth: 'none', marginTop: '20px' }}
/>

It may seem that the 'live params' lines track the LLC relatively well, with their final relative ordering being somewhat aligned with the ordering of the LLCs over multiple hyperparameters. However, varying the SGLD learning rate can give slightly different relative orderings. I'm unsure how to compare the LLCs of these models or how to identify what the 'correct' hyperparameters are for a group of models. No clear inference with Big O time can be drawn because of this.

Note that `sort_unique` and `sort_freq` data series are truncated in the above graph. They both explode later and skew the graph. Here is their full graph:

<img
  src="/images/tracr-llc/llc_sparsity_comparison_from_csv_4.png"
  alt="Full LLC vs Sparsity Comparison"
  style={{ display: 'block', width: '150%', marginLeft: '-25%', maxWidth: 'none', marginTop: '20px' }}
/>

Choosing a lower learning rate (lr) will keep LLCs from exploding but also seems to harm the LLCs of models that don't need such a low learning rate. The above choice of lr, 1e-5, seemed to be a good choice because most LLCs weren't negative and were initially sanely bounded.

Here is the same plot, but with a 1e-10 lr:

<img
  src="/images/tracr-llc/llc_sparsity_comparison_from_csv.png"
  alt="LLC vs Sparsity Comparison at lr=1e-10"
  style={{ display: 'block', width: '150%', marginLeft: '-25%', maxWidth: 'none', marginTop: '20px' }}
/>

It's clear that to get a good LLC estimate some normalization of the SGLD hyperparameters needs to happen as the models increase in parameter count. Why only some models need this is unclear. Trying with dynamic SGLD learning rates that were linear/quadratic functions of the parameter count, we found it wasn't a one-size-fits-all solution. 

# LLC of Distilled Models

It seems that distilling a Tracr model into an equal or smaller model smooths the LLC loss trace and provides more stable LLCs. It also brings all Tracr models into the same 'range' of LLC. This is done by minimizing the KL divergence between the Tracr (teacher) logits and the student logits because cross entropy wouldn't preserve the Tracr models' "correct but not confident" logits (Tracr logits are always 'correct' but don't have confident log probabilities, a result of how they are compiled). Below is a chart indicating the LLC of the original Tracr model and the LLCs of incrementally shrunk, distilled versions of the model (shrunk by a percentage along d_model and d_mlp).

Below we see Teacher LLCs over a wide range of values ~[1, 60], and when distilled, most LLCs end up between 0 and 2, with `hist` and `pair_balance` slightly negative.

Note the relative LLC ordering between tasks **changes** as we compress—`length` has a higher Teacher LLC than `sort`, but when distilled, has a lower LLC. My guess is that the Teacher LLC of `length` was an overestimate, as the Big O ordering of the distilled LLCs is closer to correct. Note: Because these models are distilled with KL from a Tracr model, their log probabilities are also not 'confident'. Distilling works because the loss landscape of a Tracr model is highly artificial.

# LLCs of Teacher Models / Progressively Shrunk Distilled Models

| Task         | Teacher | 0% Shrunk | 48% Shrunk | 95% Shrunk | ~Restored KL?     | Theoretical Big O |
|--------------|---------|-----------|------------|------------|-------------------|-------------------|
| Sort Unique  | 63.74   | 1.71      | 1.56       | 1.85       | Y                 | O(n log n)        |
| Sort         | 16.79   | 1.77      | 1.43       | 1.52       | Y                 | O(n log n)        |
| Length       | 35.89   | 0.95      | 0.77       | 0.76       | Y                 | O(1)              |
| Frac Prevs   | 1.01    | 0.81      | 0.73       | 0.79       | Y (N @95% Shrunk) | O(n)              |
| Hist         | -0.96   | 0         | -0.49      | 0.06       | N                 | O(n)              |
| Pair Balance | -6.08   | -6.00     | -9.33      | -24.62     | N                 | O(n)              |
| Sort Freq    | 425.0   | -3.36     | -2.08      | -2.72      | Y                 | O(n)              |
| Dyck-2       | NaN     | -         | -          | -          | -                 | -                 |
| Dyck-3       | NaN     | -         | -          | -          | -                 | -                 |

# SGLD Loss Trace of Teacher vs Compressed Students

**Sort Unique:**

<img
  src="/images/tracr-llc/sort_unique_shrink_comparison.png"
  alt="Sort Unique Shrink Comparison"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Sort:**

<img
  src="/images/tracr-llc/sort_shrink_comparison.png"
  alt="Sort Shrink Comparison"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Length:**

<img
  src="/images/tracr-llc/length_shrink_comparison.png"
  alt="Length Shrink Comparison"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Frac Prevs:**

<img
  src="/images/tracr-llc/frac_prevs_shrink_comparison_1.png"
  alt="Frac Prevs Shrink Comparison 1"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Hist:**

<img
  src="/images/tracr-llc/hist_shrink_comparison_1.png"
  alt="Hist Shrink Comparison 1"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Pair Balance:**

<img
  src="/images/tracr-llc/pair_balance_shrink_comparison_1.png"
  alt="Pair Balance Shrink Comparison 1"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

**Sort Freq:**

<img
  src="/images/tracr-llc/sort_freq_shrink_comparison.png"
  alt="Sort Freq Shrink Comparison"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

*Note:* I was unable to attain traces for Dyck-2 and Dyck-3 due to exploding gradients and because distilling from them is a nightmare.

# LLC vs Shrink % of Distilled Model

Looking at the stability of the LLC for different levels of compression ('shrink'), we see that for some values the LLC is noisy, while for `length`, `frac_prevs`, and `sort_unique` are remarkably stable. A more in-depth experiment, perhaps without making use of Tracr, could be beneficial. Each LLC sampling was done with 5 chains.

<img
  src="/images/tracr-llc/llc_and_kl_vs_shrink.png"
  alt="LLC and KL vs Shrink"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

# Landscape Visualization

Looking at the loss landscapes, visualized by perturbing weights along the top 2 principal components of the gradients, **we see that we can explain the LLCs of the teacher models above**. For instance, the trace of SGLD on `pair_balance` descends, and the loss landscape shows that the starting weights were not at a local minimum.

Similarly, the initial spike in the trace of `hist`'s teacher LLC is likely due to SGLD exploring directly up the wall on the side of the starting weights, leading to a huge spike in loss, while the rest of its search is over a flat loss landscape. In general these loss landscapes are much nicer than expected, which is very encouraging.

# Loss Landscapes of 'Well Behaved' Tracr Models
(`sort`, `length`, `frac_prevs`, `hist`, `pair_balance`)

<img
  src="/images/tracr-llc/Loss_Landscapes_of_Nice_Programs_1.png"
  alt="Loss Landscapes of Nice Programs 1"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

Similarly, we can examine the loss landscapes of the models that had 'badly' behaved LLCs.

# Loss Landscapes of 'Badly Behaved' Tracr Models
(`sort_freq`, `dyck-2`, `dyck-3`, `sort_unique`)

<img
  src="/images/tracr-llc/Loss_Landscapes_of_Mean_Programs.png"
  alt="Loss Landscapes of Mean Programs"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

We see that their loss landscapes are terribly sharp in comparison! If I'm understanding the practicalities of the LLC here, this should explain **why** estimating the LLC didn't work well. 

For instance, **when the SGLD lr was too high in the first plot**, the LLCs for `sort_freq` and `sort_unique` exploded. Here we can see that **they likely exploded because a high enough LR allowed SGLD to escape the sharp local minima to the very high loss plains surrounding it**. 

Furthermore, we can examine the loss landscapes of the distilled models. Distilling Tracr models into models of *equivalent* size/architecture results in landscapes that are (as expected) not nearly as sharp as their equivalent raw Tracr model.

# Loss Landscapes of *Distilled* 'Well Behaved' Programs

<img
  src="/images/tracr-llc/Loss_Landscapes_of_Distilled_Nice_Programs.png"
  alt="Loss Landscapes of Distilled Nice Programs"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

# Loss Landscapes of *Distilled* 'Badly Behaved' Programs

<img
  src="/images/tracr-llc/Loss_Landscapes_of_Distilled_Mean_Programs.png"
  alt="Loss Landscapes of Distilled Mean Programs"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>
*`sort_freq` and `sort_unique` distill into smooth landscapes whilst `dyck-2` and `dyck-3` are still contrived.*

**Note on Dyck Models:**

The Dyck Tracr models required an SGLD learning rate of 1e-12 to achieve any semblance of reasonable LLC. For this reason they were impractical to plot with other LLCs of other models. 

These were the results at 1e-10:

<img
  src="/images/tracr-llc/first-real-dyck-3_shrink_comparison.png"
  alt="First Real Dyck-3 Shrink Comparison at lr=1e-10"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

<img
  src="/images/tracr-llc/first-real-dyck-2_shrink_comparison.png"
  alt="First Real Dyck-2 Shrink Comparison at lr=1e-10"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

At 1e-12:

<img
  src="/images/tracr-llc/dyck-2_shrink_comparison.png"
  alt="Dyck-2 Shrink Comparison at lr=1e-12"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

<img
  src="/images/tracr-llc/dyck-3_shrink_comparison.png"
  alt="Dyck-3 Shrink Comparison at lr=1e-12"
  style={{ display: 'block', width: '180%', marginLeft: '-40%', maxWidth: 'none', marginTop: '20px' }}
/>

At lr=1e-20, results were similar.